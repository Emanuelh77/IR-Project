{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = stopwords.words('english')\n",
    "coll_stops = [\"also\", \"first\", \"one\", \"new\", \"year\", \"two\", \"time\", \"state\", \"school\"]\n",
    "stops.extend(coll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/Maro/Wikipedia/wikipedia_text_files_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.DataFrame()\n",
    "content['content'] = data.content.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc_low = doc.lower().replace(\"–\", \" \")\n",
    "    words = word_tokenize(doc_low)\n",
    "    words = [lemmatizer.lemmatize(w).strip() for w in words if not w in stops and wordnet.synsets(w) and w.isalpha()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [23:32<00:00, 70.81it/s]\n"
     ]
    }
   ],
   "source": [
    "words1 = [clean(x) for x in tqdm(content.content[0:100000])]\n",
    "flat_words1 = [item for sublist in words1 for item in sublist]\n",
    "cleandocs1 = [\" \".join(doc) for doc in words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [1:32:06<00:00, 18.10it/s] \n"
     ]
    }
   ],
   "source": [
    "words2 = [clean(x) for x in tqdm(content.content[100000:200000])]\n",
    "flat_words2 = [item for sublist in words2 for item in sublist]\n",
    "cleandocs2 = [\" \".join(doc) for doc in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [49:43<00:00, 33.52it/s] \n"
     ]
    }
   ],
   "source": [
    "words3 = [clean(x) for x in tqdm(content.content[200000:300000])]\n",
    "flat_words3 = [item for sublist in words3 for item in sublist]\n",
    "cleandocs3 = [\" \".join(doc) for doc in words3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [43:45<00:00, 38.08it/s] \n"
     ]
    }
   ],
   "source": [
    "words4 = [clean(x) for x in tqdm(content.content[300000:400000])]\n",
    "flat_words4 = [item for sublist in words4 for item in sublist]\n",
    "cleandocs4 = [\" \".join(doc) for doc in words4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [41:23<00:00, 40.27it/s] \n"
     ]
    }
   ],
   "source": [
    "words5 = [clean(x) for x in tqdm(content.content[400000:500000])]\n",
    "flat_words5 = [item for sublist in words5 for item in sublist]\n",
    "cleandocs5 = [\" \".join(doc) for doc in words5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [37:13<00:00, 28.22it/s] \n"
     ]
    }
   ],
   "source": [
    "words6 = [clean(x) for x in tqdm(content.content[500000:600000])]\n",
    "flat_words6 = [item for sublist in words6 for item in sublist]\n",
    "cleandocs6 = [\" \".join(doc) for doc in words6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [34:23<00:00, 48.45it/s] \n"
     ]
    }
   ],
   "source": [
    "words7 = [clean(x) for x in tqdm(content.content[600000:700000])]\n",
    "flat_words7 = [item for sublist in words7 for item in sublist]\n",
    "cleandocs7 = [\" \".join(doc) for doc in words7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [33:34<00:00, 49.64it/s] \n"
     ]
    }
   ],
   "source": [
    "words8 = [clean(x) for x in tqdm(content.content[700000:800000])]\n",
    "flat_words8 = [item for sublist in words8 for item in sublist]\n",
    "cleandocs8 = [\" \".join(doc) for doc in words8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [30:13<00:00, 55.14it/s] \n"
     ]
    }
   ],
   "source": [
    "words9 = [clean(x) for x in tqdm(content.content[800000:900000])]\n",
    "flat_words9 = [item for sublist in words9 for item in sublist]\n",
    "cleandocs9 = [\" \".join(doc) for doc in words9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [29:30<00:00, 56.47it/s] \n"
     ]
    }
   ],
   "source": [
    "words10 = [clean(x) for x in tqdm(content.content[900000:1000000])]\n",
    "flat_words10 = [item for sublist in words10 for item in sublist]\n",
    "cleandocs10 = [\" \".join(doc) for doc in words10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [26:46<00:00, 62.24it/s] \n"
     ]
    }
   ],
   "source": [
    "words11 = [clean(x) for x in tqdm(content.content[1000000:1100000])]\n",
    "flat_words11 = [item for sublist in words11 for item in sublist]\n",
    "cleandocs11 = [\" \".join(doc) for doc in words11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [24:34<00:00, 67.83it/s] \n"
     ]
    }
   ],
   "source": [
    "words12 = [clean(x) for x in tqdm(content.content[1100000:1200000])]\n",
    "flat_words12 = [item for sublist in words12 for item in sublist]\n",
    "cleandocs12 = [\" \".join(doc) for doc in words12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [23:41<00:00, 70.36it/s] \n"
     ]
    }
   ],
   "source": [
    "words13 = [clean(x) for x in tqdm(content.content[1200000:1300000])]\n",
    "flat_words13 = [item for sublist in words13 for item in sublist]\n",
    "cleandocs13 = [\" \".join(doc) for doc in words13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [23:02<00:00, 72.33it/s] \n"
     ]
    }
   ],
   "source": [
    "words14 = [clean(x) for x in tqdm(content.content[1300000:1400000])]\n",
    "flat_words14 = [item for sublist in words14 for item in sublist]\n",
    "cleandocs14 = [\" \".join(doc) for doc in words14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [21:46<00:00, 76.56it/s]  \n"
     ]
    }
   ],
   "source": [
    "words15 = [clean(x) for x in tqdm(content.content[1400000:1500000])]\n",
    "flat_words15 = [item for sublist in words15 for item in sublist]\n",
    "cleandocs15 = [\" \".join(doc) for doc in words15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [17:53<00:00, 93.11it/s] \n"
     ]
    }
   ],
   "source": [
    "words16 = [clean(x) for x in tqdm(content.content[1500000:1600000])]\n",
    "flat_words16 = [item for sublist in words16 for item in sublist]\n",
    "cleandocs16 = [\" \".join(doc) for doc in words16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62756/62756 [11:58<00:00, 87.40it/s] \n"
     ]
    }
   ],
   "source": [
    "words17 = [clean(x) for x in tqdm(content.content[1600000:1662756])]\n",
    "flat_words17 = [item for sublist in words17 for item in sublist]\n",
    "cleandocs17 = [\" \".join(doc) for doc in words17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcleandocs = cleandocs1 + cleandocs2 + cleandocs3 + cleandocs4 + cleandocs5 + cleandocs6 + cleandocs7 + cleandocs8 + cleandocs9  + cleandocs10  + cleandocs11  + cleandocs12 + cleandocs13  + cleandocs14 + cleandocs15 + cleandocs16 + cleandocs17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "allflatwords = flat_words1 + flat_words2 + flat_words3 + flat_words4 + flat_words5 + flat_words6 + flat_words7 + flat_words8 + flat_words9  + flat_words10  + flat_words11  + flat_words12 + flat_words13  + flat_words14 + flat_words15 + flat_words16 + flat_words17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = words1 + words2 + words3 + words4 + words5 + words6 + words7 + words8 + words9  + words10  + words11  + words12 + words13  + words14 + words15 + words16 + words17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [item for item in allflatwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = pd.DataFrame()\n",
    "cd['clean'] = allwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[morocco, saudi, arabia, relation, moroccan, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[anthony, united, state, post, office, anthony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[constituency, constituency, legislative, asse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[born, september, beirut, lebanon, author, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pastor, pastor, june, february, spanish, arch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean\n",
       "0  [morocco, saudi, arabia, relation, moroccan, s...\n",
       "1  [anthony, united, state, post, office, anthony...\n",
       "2  [constituency, constituency, legislative, asse...\n",
       "3  [born, september, beirut, lebanon, author, rep...\n",
       "4  [pastor, pastor, june, february, spanish, arch..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.to_csv(\"/data/ASHLEE/doc_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1662756/1662756 [1:11:20<00:00, 388.46it/s] \n"
     ]
    }
   ],
   "source": [
    "index = {i: {} for i in vocab}\n",
    "temp = {}\n",
    "for i, d in enumerate(tqdm(cd.clean)):\n",
    "    d_s = list(set(d))\n",
    "    for w in d_s:\n",
    "        frq = d.count(w)\n",
    "        temp = {i:frq}\n",
    "        index[w].update(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-d85fd0974d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "with open('index.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in index.items():\n",
    "        writer.writerow([key, {value}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/data/ASHLEE/index.json', 'w') as fp:\n",
    "    json.dump(index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index['cowhide'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88949/88949 [00:00<00:00, 305617.20it/s]\n"
     ]
    }
   ],
   "source": [
    "idf_all = []\n",
    "for x in tqdm(index):\n",
    "    n_w = len(index[x].keys())\n",
    "    idf = math.log(1662756/n_w,2)\n",
    "    idf_all.append(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df = pd.DataFrame()\n",
    "idf_df['word'] = vocab\n",
    "idf_df['idf'] = idf_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df.to_csv('idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/ASHLEE/index.csv') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    mydict = dict(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
