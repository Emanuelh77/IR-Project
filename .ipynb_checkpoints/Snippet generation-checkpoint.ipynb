{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#These are the files used in the code\n",
    "idf = pd.read_csv('idf.csv')\n",
    "max_d = pd.read_csv('max_d.csv')\n",
    "cd = pd.read_csv('doc_df.csv')\n",
    "index = pd.read_csv('index.csv', header=None)\n",
    "index.columns = ['words', 'doc_freq']\n",
    "data = pd.read_csv('wikipedia_text_files.csv')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = stopwords.words('english')\n",
    "coll_stops = [\"also\", \"first\", \"one\", \"new\", \"year\", \"two\", \"time\", \"state\", \"school\"]\n",
    "stops.extend(coll_stops)\n",
    "\n",
    "def clean(doc):\n",
    "    doc_low = doc.lower().replace(\"â€“\", \" \")\n",
    "    words = word_tokenize(doc_low)\n",
    "    words = [lemmatizer.lemmatize(w).strip() for w in words if not w in stops and wordnet.synsets(w) and w.isalpha()]\n",
    "    return words\n",
    "\n",
    "#Takes one document and creates a list containint the title a top 3 realted sentence to a given clean query\n",
    "def snippet (doc_id, q):\n",
    "    doc_tfidf = pd.DataFrame()\n",
    "    sent_doc = pd.DataFrame()\n",
    "    cosine_df = pd.DataFrame()\n",
    "    sent_doc_c = pd.DataFrame()\n",
    "    \n",
    "    #Create Frame with TF-IDF for every word in the clean doc\n",
    "    clean_list = ast.literal_eval(cd.clean[doc_id]) \n",
    "    doc_tfidf['words'] = list(set(clean_list))\n",
    "    doc_tfidf['tf-idf'] = [(ast.literal_eval(index.loc[index.words == x, 'doc_freq'].item()).get(doc_id)/ max_d.max_d[doc_id])* idf.loc[idf.word == x, 'idf'].item() for x in doc_tfidf.words]\n",
    "    \n",
    "    #Grab Full doc from corpus, since I dont keep periods\n",
    "    doc = data.loc[cd.index == doc_id, 'content'].item()\n",
    "    \n",
    "    #Tokenize into sentences and clean\n",
    "    sent_doc['sent'] = sent_tokenize(doc)\n",
    "    sent_doc['clean'] = [clean(x) for x in sent_doc.sent]\n",
    "    sent_doc_c = sent_doc[sent_doc.astype(str)['clean'] != '[]'].reset_index()\n",
    "    \n",
    "    #Create vector from query and sentence\n",
    "    cosine = []\n",
    "    for x in sent_doc_c.clean:\n",
    "        x_set = set(x)\n",
    "        q_set = set(q)\n",
    "        vector = x_set.union(q_set)\n",
    "    \n",
    "    #Calculate vectore values for both\n",
    "        q_v = []\n",
    "        s_v = []\n",
    "        for w in vector:\n",
    "            if w in q_set:\n",
    "                q_v.append(doc_tfidf.loc[doc_tfidf.words == w, 'tf-idf'].item())\n",
    "            else:\n",
    "                q_v.append(0)\n",
    "            if w in x_set:\n",
    "                s_v.append(doc_tfidf.loc[doc_tfidf.words == w, 'tf-idf'].item())\n",
    "            else:\n",
    "                s_v.append(0)\n",
    "    \n",
    "    #Calculate cosine simularity\n",
    "        c = 0\n",
    "        for i in range(len(vector)):\n",
    "            c += q_v[i] * s_v[i]\n",
    "        cosine.append(c / math.sqrt((math.pow(sum(q_v), 2)) * (math.pow(sum(s_v), 2))))\n",
    "    cosine_df['sim'] = cosine\n",
    "    cosine_df = cosine_df.sort_values(by=['sim'], ascending=False).reset_index()\n",
    "    sent_pos = list(cosine_df[0:3]['index'])\n",
    "    \n",
    "    #Put together the snippet as a list to return\n",
    "    snip = []\n",
    "    para = \"\"\n",
    "    snip.append(data.title[doc_id])\n",
    "    for x in sent_pos:\n",
    "        para = para + sent_doc_c.sent[x]\n",
    "    snip.append(para)\n",
    "    return snip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lackey_moth',\n",
       " 'They eat the young foliage of the tree and moult several times as they grow larger.The larvae feed mainly on trees and shrubs from within their tents.When ready to pupate they drop to the ground and undergo metamorphosis, each forming a pupa sandwiched between leaves of plants growing beneath the tree.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet(1085632,['pear','tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
